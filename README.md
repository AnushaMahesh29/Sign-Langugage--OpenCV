# Sign-Langugage--OpenCV

## About The Project
 Sign language is a vital communication tool for the Deaf and hard-of-hearing communities,
 enabling them to convey thoughts, emotions, and information through visual gestures.
 There is a need for automated systems to accurately recognize and interpret sign language
 gestures to improve communication with those unfamiliar with sign language.
 
 Our model is trained on the ISL dataset consisting of 42,000 images representing 35 distinct
 sign language gestures, each corresponding to unique characters - A to Z and numbers - 1 to 9.
 
 Dataset Link:https://www.kaggle.com/datasets/prathumarikeri/indian-sign-language-isl
 
 #### The images are reduced to 3500, 100 images in each directory.
 
 ## Objectives
 - Utilize and Enhance a Comprehensive Dataset
 - Implement Effective Preprocessing and Feature Extraction
 - Deep Learning model(CNN) and interpretation using XAI
 - Real Time Prediction - Image to Text, Text to Speech

## Dependencies

1. Python 3.x
2. Libraries:
   - os
   - numpy
   - pandas
   - sklearn
   - OpenCV
   - TensorFlow/Keras
   - mediapipe
   - gtts

## Usage

1. Create the repository
2. Install the dependencies
3. Run the Jupyter Notebook python.ipynb to execute the code step by step.
   

 
 
